---
title: "HW1_207"
author: "RafaelCatoiaPulgrossi"
date: "2023-04-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) ; library(ggplot2)
```

# Ex 1

```{r}
posterior_prop <- function(theta){
  
  return(
    (theta^3)*(1-theta)^13+
      (theta^4)*(1-theta)^12+
      (theta^5)*(1-theta)^11
  )
}

parametric_space <- seq(0.001,1,0.001)

plot(parametric_space,posterior_prop(theta=parametric_space),
     type='l',ylab='proportional to the density')
```


# Ex 5

## a 
```{r}
y <- c(43,44,45,46.5,47.5)
M=1000

parametric_space <- seq(0,100,1/M)

q_theta_given_y = function(x,theta){
  result <- 1
  for ( i in 1:length(x)){
    result = result * (1/(1+(x[i]-theta)^2))
  }
  return(result/100)
}


## unormalized density
unormalized_density <- q_theta_given_y(y,parametric_space)

## summing the heights of the bins
normalizing_constant <-  sum(unormalized_density/M)

#plotting the normalized density
plot(parametric_space,
     unormalized_density/normalizing_constant,
     type='l',
     ylab = 'normalized density')

normalized_density <-  unormalized_density/normalizing_constant
```

## b
```{r}
sample_theta <- sample(parametric_space,prob = normalized_density,size = 1000)
hist(sample_theta)
```

## c
```{r}
set.seed(57)
predictive_distribution <- rcauchy(1000,scale = 1,location = sample_theta)
plot(density(predictive_distribution))

plot(density(predictive_distribution),xlim=c(30,60))
abline(v=mean(y),col='forestgreen')
mean(predictive_distribution) ; sd(predictive_distribution)
```




# Ex 6

Here we are implementing the Gibbs Sampler.


```{r Gibbs}
x_given_y <- function(constant,y){
  a <- max(0,y-constant)
  b <- min(y+constant,1)
  return(runif(1,a,b))
}

y_given_x <- function(constant,x){
  a <- max(0,x-constant)
  b <- min(x+constant,1)
  return(runif(1,a,b))
}


gibbs_ex6 <- function(B=1000,constant){
  set.seed(1000)
  samples <- data.frame(x=NA,y=NA)
  new_x <- 0.5 #initial value for x
  for(i in 1:(B)){
    #calculating p(y|x=new_x)
    new_y <- y_given_x(x = new_x,constant = constant) 
    #calculating p(x|y=new_y)
    new_x <- x_given_y(y = new_y,constant = constant)
    samples <-rbind(samples, c(new_x,new_y))
  }
  return(samples[-1,])
}

sample_0.3 <- gibbs_ex6(B = 1000,0.3)
sample_0.05 <- gibbs_ex6(B = 1000,0.05)
sample_0.01 <- gibbs_ex6(B = 1000,0.01)
```


## For C=0.3
```{r Gibbs}
par(mfrow=c(1,2))
constant<-0.3
plot(sample_0.3$x,type='l',col=scales::alpha('deepskyblue',0.5))
lines(sample_0.3$y,type='l',col=scales::alpha('firebrick',0.5))
plot(sample_0.3)
abline(constant,1,col='deepskyblue')
abline(-constant,1,col='deepskyblue')
abline(0,1,col='red')
#acf(sample_0.3$x)
#acf(sample_0.3$y)
```

## For C=0.05
```{r Gibbs}
constant<-0.05
plot(sample_0.05$x,type='l',col=scales::alpha('deepskyblue',0.5))
lines(sample_0.05$y,type='l',col=scales::alpha('firebrick',0.5))
plot(sample_0.05)
abline(constant,1,col='blue')
abline(-constant,1,col='blue')
abline(0,1,col='blue')
#acf(sample_0.05$x)
#acf(sample_0.05$y)
```


## For C=0.01
```{r Gibbs}
constant<-0.01
plot(sample_0.01$x,type='l',col=scales::alpha('deepskyblue',0.5))
lines(sample_0.01$y,type='l',col=scales::alpha('firebrick',0.5))
plot(sample_0.01)
abline(constant,1,col='deepskyblue')
abline(-constant,1,col='deepskyblue')
abline(0,1,col='red')
#acf(sample_0.01$x)
#acf(sample_0.01$y)
par(mfrow=c(1,1))
```




# Ex 8 - 3.4

```{r}
nsim <- 10000
posterior_simulation <- data.frame(
  p0 = rbeta(nsim,40,636),
  p1 = rbeta(nsim,23,569)
)


posterior_simulation = 
  posterior_simulation %>% 
  mutate(odds_p0=p0/(1-p0),
         odds_p1=p1/(1-p1),
         odds_ratio = odds_p1/odds_p0)

posterior_simulation %>% 
  summarise(
    Min = min(odds_ratio),
    Mean = mean(odds_ratio),
    Median = median(odds_ratio),
    SD = sd(odds_ratio),
    Max = max(odds_ratio)) %>% 
  knitr::kable() %>% kableExtra::kable_styling()
```

```{r}
posterior_simulation %>% 
  ggplot(aes(x=odds_ratio))+
  geom_histogram() + 
  theme_bw()
```


```{r}
y_bar_control = 39/674
y_bar_treatment = 22/680

#Posterior probability 
sum(posterior_simulation$p0 > y_bar_control)/nsim

sum(posterior_simulation$p1 > y_bar_treatment)/nsim

sum(posterior_simulation$odds_ratio < 1)/nsim
```


## Now changing the sample size


```{r}
nsim <- 10000
posterior_simulation <- data.frame(
  p0 = rbeta(nsim,3,35),
  p1 = rbeta(nsim,2,35)
)


posterior_simulation = 
  posterior_simulation %>% 
  mutate(odds_p0=p0/(1-p0),
         odds_p1=p1/(1-p1),
         odds_ratio = odds_p1/odds_p0)

posterior_simulation %>% 
  summarise(
    Min = min(odds_ratio),
    Mean = mean(odds_ratio),
    Median = median(odds_ratio),
    SD = sd(odds_ratio),
    Max = max(odds_ratio)) %>% 
  knitr::kable() %>% kableExtra::kable_styling()
```

```{r}
posterior_simulation %>% 
  ggplot(aes(x=odds_ratio))+
  geom_histogram() + 
  theme_bw()
```


```{r}
y_bar_control = 2/34
y_bar_treatment = 1/34

#Posterior probability 
sum(posterior_simulation$p0 > y_bar_control)/nsim

sum(posterior_simulation$p1 > y_bar_treatment)/nsim

sum(posterior_simulation$odds_ratio < 1)/nsim
```

# Ex 9 


## a)

Lets use as non-informative prior $p(\mu,\sigma) \propto \frac{1}{\sigma^2}$, our data-sampling distribution is given by $p(y | \mu,\sigma^2)\sim N(\mu,\sigma^2)$.

Thus, our posterior distribution is going to be:

$$p(\mu,\sigma^2) = p(\mu | \sigma^2 , y ) p(\sigma^2 | y) $$
being $\mu | \sigma^2 , y\sim N(\bar{y},\frac{\sigma^2}{n}$ and $\sigma^2 | y \sim Inv \chi^2 (n-1, s^2)$

```{r}
y = c(10,10,12,11,9)
ybar = mean(y)
n = length(y)
S2 = var(y)

set.seed(1234)

generate_posterior_untruncated <- function(y){
  ybar = mean(y)
  n = length(y)
  S2 = var(y)
  sigma2 <- invgamma::rinvchisq(1,n-1,S2)
  mu <- rnorm(1,mean=ybar,sd=sqrt(sigma2))
  
  return(data.frame(mu,sigma2))
}

#now, drawing a sample from \mu,\sigma^2
generate_posterior_untruncated(y)


density_posterior_untruncated <- function(mu,sigma2,y){
  ybar = mean(y)
  S2 = var(y)
  n=length(y)
  return(
    dnorm(mu,mean = ybar,sd=sqrt(sigma2)/sqrt(n))*
      invgamma::dinvchisq(sigma2,df = n-1,ncp = S2)
  )
}
```

## b)
Now our likelihood is different, so does he posterior which is given by: $p(\mu,\sigma^2 | y ) \propto \frac{1}{\sigma^2}\prod_{i=1}^{n} \left[ \Phi(\frac{y_i-\mu+0.5}{\sigma}) - \Phi(\frac{y_i+\mu - 0.5}{\sigma}) \right]$

being $\mu | \sigma^2 , y\sim N(\bar{y},\frac{\sigma^2}{n})$ and $\sigma^2 | y \sim Inv \chi^2 (n-1, s^2)$


```{r}
#proportional posterior density
prop_density_post_truncated <- function(y,sigma2,mu){
  prop_density <- 1
  for(i in 1:length(y)){
    
    prop_density = prop_density *
      (pnorm(y[i]+0.5,mean = mu,sd = sqrt(sigma2))  -
         pnorm(y[i]-0.5,mean = mu,sd = sqrt(sigma2)))
  }
  return((1/sigma2)*prop_density)
}

#### MCMC -------------------------------------------------------
## Metropolis Hastings ------------------------------------------
# candidate will be the posterior without truncation

n_it <- 1000 #number of samples to be generated
step <- 10
burn_in = 10000
total_samples = burn_in + n_it*step
init_value <- c(10,4) # initial value of theta
accepted=0
post_samp_truncated <- 
  data.frame(mu=rep(NA,n_it),
             sigma2=rep(NA,n_it))

## Let's use as candidate the posterior
## pretending unrounded measurements
post_samp_truncated[1,]<-init_value
for (i in 2:total_samples){
  candidate <- generate_posterior_untruncated(y=y)
  
  ## Calculating R
  #numerators
  num1 = prop_density_post_truncated(
    y = y,
    sigma2 = candidate$sigma2,
    mu = candidate$mu)
  
  num2 = density_posterior_untruncated(
    y = y,
    mu=candidate$mu,
    sigma2 = candidate$sigma2
  )
  
  #denominators
  den1 = prop_density_post_truncated(
    y = y,
    sigma2 = post_samp_truncated[i-1,]$sigma2,
    mu = post_samp_truncated[i-1,]$mu)
  
  den2 = density_posterior_untruncated(
    y = y,
    mu=post_samp_truncated[i-1,]$mu,
    sigma2 = post_samp_truncated[i-1,]$sigma2
  )
  
  ## r
  ## using the log since the densities can be very small
  r=min(1,exp(log(num1)+log(num2)-(log(den1)+log(den2))))
  
  if(runif(1)>r){
    # if not accepted, we will start the new iteration with the same point
    post_samp_truncated[i,] <- post_samp_truncated[i-1,]
  } else {
    # if accepted, we will move our chain to the candidate point and start the 
    # next iteration from this point
    post_samp_truncated[i,] <- candidate
    accepted<-accepted+1
  }
}

## acceptance rate = 
accepted/total_samples

## brushing the sample
post_sample_clean <- post_samp_truncated[-c(1:burn_in),][seq(1,total_samples-burn_in,by=step),]

par(mfrow=c(2,1))
plot(post_sample_clean$mu,type='l')
plot(post_sample_clean$sigma2,type='l')
par(mfrow=c(1,1))
acf(post_sample_clean)
```


## c)

```{r}
sample_posterior_untruncated <- data.frame(
  mu=rep(NA,n_it),
  sigma2=rep(NA,n_it)
)

for(i in 1:n_it){
  sample_posterior_untruncated[i,] <- generate_posterior_untruncated(y=y)
}


posterior_samples <- data.frame(
  mu_untruncated = sample_posterior_untruncated$mu,
  mu_truncated = post_sample_clean$mu,
  sigma2_untruncated = sample_posterior_untruncated$sigma2,
  sigma2_truncated = post_sample_clean$sigma2
)


data.frame(
  Mean =  posterior_samples %>% 
    summarise(Untruncated = mean(mu_untruncated),
              Truncated = mean(mu_truncated)) %>% t(),
  SD =posterior_samples %>% 
    summarise(Untruncated = sd(mu_untruncated),
              Truncated = sd(mu_truncated)) %>% t(),
  Percentile_0.5 =  posterior_samples %>% 
    summarise(Untruncated = quantile(mu_untruncated,0.05),
              Truncated = quantile(mu_truncated,0.1)) %>% t(),
  Percentile_1 = posterior_samples %>% 
    summarise(Untruncated = quantile(mu_untruncated,0.1),
              Truncated = quantile(mu_truncated,0.05)) %>% t(),
  Percentile_25 =   posterior_samples %>% 
    summarise(Untruncated = quantile(mu_untruncated,0.25),
              Truncated = quantile(mu_truncated,0.25)) %>% t(),
  Percentile_50 = posterior_samples %>% 
    summarise(Untruncated = quantile(mu_untruncated,0.5),
              Truncated = quantile(mu_truncated,0.5)) %>% t()
) %>% knitr::kable(caption = 'For mu') %>% kableExtra::kable_styling()
```


```{r}
data.frame(
  Mean =  posterior_samples %>% 
    summarise(Untruncated = mean(sigma2_untruncated),
              Truncated = mean(sigma2_truncated)) %>% t(),
  SD =posterior_samples %>% 
    summarise(Untruncated = sd(sigma2_untruncated),
              Truncated = sd(sigma2_truncated)) %>% t(),
  Percentile_0.5 =  posterior_samples %>% 
    summarise(Untruncated = quantile(sigma2_untruncated,0.05),
              Truncated = quantile(sigma2_truncated,0.1)) %>% t(),
  Percentile_1 = posterior_samples %>% 
    summarise(Untruncated = quantile(sigma2_untruncated,0.1),
              Truncated = quantile(sigma2_truncated,0.05)) %>% t(),
  Percentile_25 =   posterior_samples %>% 
    summarise(Untruncated = quantile(sigma2_untruncated,0.25),
              Truncated = quantile(sigma2_truncated,0.25)) %>% t(),
  Percentile_50 = posterior_samples %>% 
    summarise(Untruncated = quantile(sigma2_untruncated,0.5),
              Truncated = quantile(sigma2_truncated,0.5)) %>% t()
) %>% knitr::kable(caption = 'For sigma2') %>% kableExtra::kable_styling()
```

Now, the density plot:

```{r}
#density untruncated
mu_grid <- seq(5,15,0.1)
sigma2_grid <- seq(0.01,10,0.1)
density_values <- matrix(NA,nrow=length(mu_grid),ncol=length(sigma2_grid)) 

for(i in 1:length(mu_grid)){
  for(j in 1:length(sigma2_grid)){
  cat(paste(i,j),'\n')
    density_values[i,j] <- density_posterior_untruncated(
      y = y,
      mu = mu_grid[i],
      sigma2 = sigma2_grid[j]
    )
  }
}

contours <- c(0.001,.01,seq(.1,.9,.1))
contour(
  x = mu_grid,
  y = log(sqrt(sigma2_grid)),
  z = density_values,
  levels=contours
  )  

# sample_posterior_untruncated$density <-density_untruncated
# sample_posterior_untruncated %>% 
#   ggplot2::ggplot(aes(x=mu,y=sigma2))+geom_density2d()
```


```{r}
#density truncad
density_truncated <- rep(NA,nrow(post_sample_clean))
a
for(i in 1:nrow(density_truncated)){
  density_truncated[i] <- prop_density_post_truncated(
    y = y,
    mu = post_sample_clean$mu[i],
    sigma2 = post_sample_clean$sigma2[i]
  )
}


post_sample_clean %>% 
  ggplot2::ggplot(aes(x=mu,y=sigma2))+geom_density2d()

```



## d)




# Ex 10 

```{r}
# proportional to the posterior distribution that we
# are going to draw samples from
# Witout using log
propto_posterior <- function(y=1.5,theta){
  numerator <- exp(-(1/2)*(y-theta)^2)
  denominator <- (1+theta^2)
  return(numerator/denominator)
}

# Using log
propto_posterior <- function(y=1.5,theta){
 numerator <- -(1/2)*(y-theta)^2
 denominator <- -log(1+theta^2)
 return(exp(numerator+denominator))
}

randWalk_MH_diagnostics <- function(posterior_sample){
  lindseys_density  <- density(posterior_sample)
  a <- round(min(lindseys_density$x),3) ; b <-round(max(lindseys_density$x),3)
  grid<- seq(a,b,0.001)
  par(mfrow=c(1,3))
  plot(grid,propto_posterior(theta = grid,y=1.5),type='l',lty=2,lwd=1.5)
  lines(density(posterior_sample),type='l',col='deepskyblue',lwd=1.5)
  legend(a,0.5,legend = c('prop_posterior','sample_density'),
         col=c('black','deepskyblue'),lwd = c(1.5,1.5), lty=c(2,1))
  plot(posterior_sample,type = 'l')
  acf(posterior_sample)
  par(mfrow=c(1,3))
}


n_it <- 10000 #number of samples to be generated
init_value <- 0.5 # initial value of theta 
proposal_dist <- function(theta){rnorm(1,theta,sd=2)} # we are going to use the normal distribution as proposal
post_sample <- rep(NA,n_it) #creating the vector that will store the samples
post_sample[1]=init_value #the first draw of the posterior is the initial value
accepted=0 # counting the accepted values to see the acceptance rate
# initializing the iteration
for (i in 2:n_it){
  candidate <- proposal_dist(post_sample[i-1]) 
  
  #calculating acceptance probability of the current point
  r = min(1,propto_posterior(theta = candidate) /
            propto_posterior(theta = post_sample[i-1]))
  
  #accepting the candidates with probability r
  if(runif(1)>r){
    # if not accepted, we will start the new iteration with the same point
    post_sample[i] <- post_sample[i-1]
  } else {
    # if accepted, we will move our chain to the candidate point and start the 
    # next iteration from this point
    post_sample[i] <- candidate
    accepted<-accepted+1
  }
}

accepted/n_it


randWalk_MH_diagnostics(post_sample)

summary(post_sample)
```


```{r}

n_it <- 1000 #number of samples to be generated
init_value <- 0 # initial value of theta
step <- 5
burn_in = 10000
total_samples = burn_in + n_it*step
proposal_dist <- function(theta){rnorm(1,theta,sd=2)} # we are going to use the normal distribution as proposal
post_sample <- rep(NA,total_samples) #creating the vector that will store the samples
post_sample[1]=init_value #the first draw of the posterior is the initial value
accepted=0 # counting the accepted values to see the acceptance rate

# initializing the iteration
for (i in 2:total_samples){
  candidate <- proposal_dist(post_sample[i-1]) 
  
  #calculating acceptance probability of the current point
  r = min(1,propto_posterior(theta = candidate) /
            propto_posterior(theta = post_sample[i-1]))
  
  #accepting the candidates with probability r
  if(runif(1)>r){
    # if not accepted, we will start the new iteration with the same point
    post_sample[i] <- post_sample[i-1]
  } else {
    # if accepted, we will move our chain to the candidate point and start the 
    # next iteration from this point
    post_sample[i] <- candidate
    accepted<-accepted+1
  }
}

#acceptance rate
accepted/total_samples


post_sample_clean <- post_sample[-c(1:burn_in)][seq(1,length(post_sample)-burn_in,step)]

#verifying if the length of the draw matches with the desired number
post_sample_clean %>% length()
randWalk_MH_diagnostics(post_sample_clean)

post_sample_clean %>% summary()
post_sample_clean %>% sd()
```

